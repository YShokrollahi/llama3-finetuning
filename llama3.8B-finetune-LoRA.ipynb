{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12a9a3df-6dfe-48d2-80be-1b25f6a660bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The current process just got forked\")\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "\n",
    "from transformers import TrainerCallback, TrainingArguments, Trainer\n",
    "from typing import Dict, Union\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ceb319-c6d5-4d23-8973-ae928b030f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PubMedDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.abstracts = self.load_abstracts(file_path)\n",
    "        print(f\"File path: {file_path}\")\n",
    "        print(f\"File exists: {os.path.exists(file_path)}\")\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"File size: {os.path.getsize(file_path)} bytes\")\n",
    "        print(f\"Number of abstracts loaded: {len(self.abstracts)}\")\n",
    "        if len(self.abstracts) > 0:\n",
    "            print(f\"First few characters of the first abstract: {self.abstracts[0][:200]}\")\n",
    "        else:\n",
    "            print(\"No abstracts were loaded. Check the file content and format.\")\n",
    "\n",
    "    def load_abstracts(self, file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            abstracts = [abs.strip() for abs in content.split('###') if abs.strip()]\n",
    "            return abstracts\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading abstracts: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.abstracts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        abstract = self.abstracts[idx]\n",
    "        encoded = self.tokenizer.encode_plus(\n",
    "            abstract,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'].flatten(),\n",
    "            'attention_mask': encoded['attention_mask'].flatten()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "974fdbc2-7cd9-4b7a-ae3e-843c4a33ef0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 4/4 [00:44<00:00, 11.14s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: meta-llama/Meta-Llama-3-8B\n",
      "Model dtype: torch.bfloat16\n",
      "Tokenizer vocabulary size: 128256\n",
      "Padding token: <|end_of_text|>\n",
      "Padding token ID: 128001\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "auth_token = \"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\"\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=auth_token)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    # Try loading with bfloat16 first\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            token=auth_token,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    except RuntimeError:\n",
    "        # If bfloat16 fails, try with float16\n",
    "        print(\"Loading with bfloat16 failed, trying float16...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            token=auth_token,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    \n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    print(f\"Model loaded: {model_name}\")\n",
    "    print(f\"Model dtype: {model.dtype}\")\n",
    "    print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"Padding token: {tokenizer.pad_token}\")\n",
    "    print(f\"Padding token ID: {tokenizer.pad_token_id}\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"ImportError: {e}\")\n",
    "    print(\"Try installing the required libraries with:\")\n",
    "    print(\"pip install accelerate transformers torch\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "764661bc-8f79-424b-a7ca-de77c587718c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,  # Increased rank\n",
    "    lora_alpha=4,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2c09561-f9a1-4752-b736-2d5db7cbdcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path: pub-200k/train.txt\n",
      "File exists: True\n",
      "File size: 357620996 bytes\n",
      "Number of abstracts loaded: 190657\n",
      "First few characters of the first abstract: 24491034\n",
      "BACKGROUND\tThe emergence of HIV as a chronic condition means that people living with HIV ar\n",
      "Loaded 190657 abstracts for training\n",
      "\n",
      "Example of tokenized input:\n",
      "Input IDs (first 10): [128000, 13719, 21056, 1958, 198, 82522, 33026, 49179, 315, 23495]\n",
      "Attention Mask (first 10): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Decoded text (first 100 characters):\n",
      "<|begin_of_text|>24491034\n",
      "BACKGROUND\tThe emergence of HIV as a chronic condition means that people l\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PubMedDataset('pub-200k/train.txt', tokenizer)\n",
    "print(f\"Loaded {len(train_dataset)} abstracts for training\")\n",
    "\n",
    "if len(train_dataset) > 0:\n",
    "    example_item = train_dataset[0]\n",
    "    input_ids = example_item['input_ids']\n",
    "    attention_mask = example_item['attention_mask']\n",
    "    \n",
    "    print(\"\\nExample of tokenized input:\")\n",
    "    print(\"Input IDs (first 10):\", input_ids[:10].tolist())\n",
    "    print(\"Attention Mask (first 10):\", attention_mask[:10].tolist())\n",
    "    \n",
    "    decoded_text = tokenizer.decode(input_ids)\n",
    "    print(\"\\nDecoded text (first 100 characters):\")\n",
    "    print(decoded_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7507cbbf-9902-4d92-a52f-50ddbcbf058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,  # Reduced batch size\n",
    "    gradient_accumulation_steps=4,  # Increased gradient accumulation\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    learning_rate=1e-4,  # Slightly reduced learning rate\n",
    "    max_grad_norm=0.3,  # Reduced max gradient norm for stability\n",
    "    remove_unused_columns=False,  # Important for some LoRA setups\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60ae1758-56ce-4bbe-8148-789104f857e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yshokrollahi/.local/lib/python3.11/site-packages/accelerate/accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class CustomCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.training_loss = 0.0\n",
    "        self.logging_steps = 0\n",
    "\n",
    "    def on_log(self, args: TrainingArguments, state, control, logs: Dict[str, float] = None, **kwargs):\n",
    "        if state.is_local_process_zero:\n",
    "            if 'loss' in logs:\n",
    "                self.training_loss += logs['loss']\n",
    "                self.logging_steps += 1\n",
    "            \n",
    "            if state.global_step % args.logging_steps == 0:\n",
    "                avg_loss = self.training_loss / self.logging_steps if self.logging_steps > 0 else 0\n",
    "                print(f\"Step {state.global_step}: \"\n",
    "                      f\"Loss: {avg_loss:.4f}, \"\n",
    "                      f\"Learning Rate: {logs.get('learning_rate', 0):.2e}\")\n",
    "                self.training_loss = 0.0\n",
    "                self.logging_steps = 0\n",
    "\n",
    "    def on_evaluate(self, args: TrainingArguments, state, control, metrics: Dict[str, float] = None, **kwargs):\n",
    "        if state.is_local_process_zero:\n",
    "            print(f\"Evaluation at step {state.global_step}:\")\n",
    "            for key, value in metrics.items():\n",
    "                print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Assuming you've already defined your model, tokenizer, dataset, and training arguments\n",
    "\n",
    "# Create the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "# Add the custom callback to the trainer\n",
    "custom_callback = CustomCallback()\n",
    "trainer.add_callback(custom_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eea371d8-aa7c-4ed6-bf04-0bfb91586732",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:1854\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1860\u001b[0m ):\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2744\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2742\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2743\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2746\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/accelerate/accelerator.py:2155\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfe29b04-f5dd-47ed-abd3-e3584c7a7a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.50s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "Summarize the main findings of recent studies on climate change: what are the main impacts of climate change in the areas of land use, water, biodiversity, and human health?\n",
      "The main impacts of climate change on land use, water, biodiversity, and human health include:\n",
      "Land use:\n",
      "Rising temperatures and changing\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "def load_model_and_tokenizer(base_model_name, peft_model_path):\n",
    "    # Load the base model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "    # Load the LoRA weights\n",
    "    model = PeftModel.from_pretrained(model, peft_model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_prediction(model, tokenizer, prompt, max_new_tokens=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Load the model and tokenizer\n",
    "base_model_name = \"meta-llama/Meta-Llama-3-8B\"  # Replace with your base model name\n",
    "peft_model_path = \"./results/checkpoint-10\"  # Replace with your LoRA checkpoint path\n",
    "model, tokenizer = load_model_and_tokenizer(base_model_name, peft_model_path)\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Summarize the main findings of recent studies on climate change:\"\n",
    "generated_text = generate_prediction(model, tokenizer, prompt)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92bcf2de-4368-42fb-a866-d7c02c3b6e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Summarize the key points of a recent medical study on diabetes:\n",
      "Generated text: Summarize the key points of a recent medical study on diabetes: the 3-year Diabetes Prevention Program (DPP) trial\n",
      "The 3-year Diabetes Prevention Program (DPP) trial, which included 3,234 adults with impaired glucose tolerance, was conducted to determine whether intensive lifestyle intervention or metformin\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What are the main symptoms of COVID-19?\n",
      "Generated text: What are the main symptoms of COVID-19? The main symptoms of COVID-19 are fever, cough and tiredness. Other symptoms may include loss of smell and taste, headache, sore throat, muscle pain, nasal congestion, nausea, diarrhoea and vomiting. Some people may develop more severe\n",
      "\n",
      "Prompt: Explain the process of photosynthesis in simple terms:\n",
      "Generated text: Explain the process of photosynthesis in simple terms: What is the chemical formula for photosynthesis? Why is it important?\n",
      "What is photosynthesis? What are its products? How does it differ from cellular respiration? What are the differences between C3, C4, and CAM photosynthesis?\n",
      "What\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Summarize the key points of a recent medical study on diabetes:\",\n",
    "    \"What are the main symptoms of COVID-19?\",\n",
    "    \"Explain the process of photosynthesis in simple terms:\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated_text = generate_prediction(model, tokenizer, prompt)\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2fc37d6-3bdd-44ee-92ec-77deb707277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.87s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Mean initial clinical activity score was 4.75 1.2 and 5 1.3 for group I and group II before treatment ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model output:\n",
      "Mean initial clinical activity score was 4.75 1.2 and 5 1.3 for group I and group II before treatment, respectively. The mean clinical activity score was 1.75 1.1 and 1.7 1.1 for group I and group II after treatment, respectively. The mean clinical activity score was 2.25 1.1\n",
      "\n",
      "LoRA model output:\n",
      "Mean initial clinical activity score was 4.75 1.2 and 5 1.3 for group I and group II before treatment, respectively (p = 0.17).\n",
      "Mean initial clinical activity score was 4.75 1.2 and 5 1.3 for group I and group II before treatment, respectively (p = 0.17).\n",
      "Mean initial\n"
     ]
    }
   ],
   "source": [
    "def compare_models(base_model, lora_model, tokenizer, prompt):\n",
    "       print(f\"Prompt: {prompt}\")\n",
    "       \n",
    "       base_output = generate_prediction(base_model, tokenizer, prompt)\n",
    "       print(\"Base model output:\")\n",
    "       print(base_output)\n",
    "       \n",
    "       lora_output = generate_prediction(lora_model, tokenizer, prompt)\n",
    "       print(\"\\nLoRA model output:\")\n",
    "       print(lora_output)\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "prompt = \"Mean initial clinical activity score was 4.75 1.2 and 5 1.3 for group I and group II before treatment ,\"\n",
    "compare_models(base_model, model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb0567-bec8-423d-8967-9b2ef9685b37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yshokrollahi",
   "language": "python",
   "name": "yshokrollahi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
